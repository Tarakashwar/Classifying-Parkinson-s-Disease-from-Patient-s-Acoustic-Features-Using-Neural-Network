{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFk-XW-kBCDS",
        "outputId": "e77c0a89-efd2-4d47-f1b8-c388defc098c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing important files....\n",
        "import io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn import tree\n",
        "from sklearn.svm import SVC\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import linear_model\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn import metrics\n",
        "from xgboost import XGBRegressor, plot_tree\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.inspection import permutation_importance\n",
        "from statistics import mean, stdev\n",
        "from sklearn import tree\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tXrsldLXBEoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Dataset/Parkinson disease.csv')\n"
      ],
      "metadata": {
        "id": "huFm6gO-BEmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_name=['MDVP:Fo(Hz)','MDVP:Fhi(Hz)','MDVP:Flo(Hz)','MDVP:Jitter(%)','MDVP:Jitter(Abs)','MDVP:RAP','MDVP:PPQ','Jitter:DDP','MDVP:Shimmer','MDVP:Shimmer(dB)','Shimmer:APQ3','Shimmer:APQ5','MDVP:APQ','Shimmer:DDA','NHR','HNR','RPDE','DFA','spread1','spread2','D2','PPE']\n",
        "_#Defining features(X) and labels(Y)....\n",
        "X = df.drop(['status','name'],axis =1).values\n",
        "y = df['status'].values\n",
        "\n"
      ],
      "metadata": {
        "id": "oBt_OUBABEjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking is any nan value available or not\n",
        "np.any(np.isnan(X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5B1bu4SBEg_",
        "outputId": "418077bf-2091-4438-f0ba-c1c7a77dc336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_scaled = StandardScaler().fit_transform(X)"
      ],
      "metadata": {
        "id": "_t69PtWhBEey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_scaled.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7uyncwnBEcr",
        "outputId": "9937e359-c32c-4654-cac2-db5788eaa2f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(195, 22)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y == 1)))\n",
        "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y== 0)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EikYFnVUBEZ8",
        "outputId": "a482f4b1-5750-4b5e-fd8b-64d3d47260bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before OverSampling, counts of label '1': 147\n",
            "Before OverSampling, counts of label '0': 48 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "sm = SMOTE(random_state = 2)\n",
        "X_res, y_res = sm.fit_resample(X_scaled, y.ravel())\n",
        "\n",
        "print('After OverSampling, the shape of train_X: {}'.format(X_res.shape))\n",
        "print('After OverSampling, the shape of train_y: {} \\n'.format(y_res.shape))\n",
        "\n",
        "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_res == 1)))\n",
        "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_res == 0)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6C7VwsBBEXW",
        "outputId": "e90f8926-9abd-4288-8c43-a634d622a425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After OverSampling, the shape of train_X: (294, 22)\n",
            "After OverSampling, the shape of train_y: (294,) \n",
            "\n",
            "After OverSampling, counts of label '1': 147\n",
            "After OverSampling, counts of label '0': 147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train and test data set split.....\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "EmhZVozeBwpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "components=1\n",
        "lda = LDA(n_components=2)\n",
        "X_train = lda.fit_transform(X_train, y_train)\n",
        "X_test = lda.transform(X_test)"
      ],
      "metadata": {
        "id": "lkiqpMRDBEVa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "91c131dd-d982-4b9a-fbb4-052987fdc5fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-0376ce67fced>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLDA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    882\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_components\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    609\u001b[0m                     \u001b[0;34m\"n_components cannot be larger than min(n_features, n_classes - 1).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m                 )\n",
            "\u001b[0;31mValueError\u001b[0m: n_components cannot be larger than min(n_features, n_classes - 1)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "Nl3lkLzi59ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DTC= DecisionTreeClassifier(random_state = 11, max_features = \"auto\",max_depth = None)\n",
        "\n",
        "estimator= AdaBoostClassifier(base_estimator = DTC)\n",
        "# defining parameter range\n",
        "\n",
        "param_grid ={\n",
        "    'n_estimators': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 20],\n",
        "    'learning_rate': [(0.97 + x / 100) for x in range(0, 8)],\n",
        "    'algorithm': ['SAMME', 'SAMME.R']\n",
        "}\n",
        "random_estimator = RandomizedSearchCV(estimator = estimator,\n",
        "                                   param_distributions = param_grid,\n",
        "                                   refit = True, verbose = 3\n",
        "                                  )\n",
        " #fitting the model for grid search\n",
        "random_estimator.fit(X_train, y_train)\n",
        "# print best parameter after tuning\n",
        "print(random_estimator.best_params_)\n",
        "\n",
        "# print how our model looks after hyper-parameter tuning\n",
        "print(random_estimator.best_estimator_)\n",
        "random_estimator_predictions = random_estimator.predict(X_test)\n",
        "\n",
        "# print classification report\n",
        "print(classification_report(y_test, random_estimator_predictions))\n"
      ],
      "metadata": {
        "id": "b7zyZadA55S6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name= \"ADA(LDA_Tuned)\"\n",
        "#model = LogisticRegression(C=0.06, max_iter=365, solver='liblinear', warm_start=True)\n",
        "\n",
        "\n",
        "#model = SVC(C=1000, gamma=0.001,kernel= 'rbf')\n",
        "\n",
        "#model =DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_leaf=50)\n",
        "\n",
        "\n",
        "#model = RandomForestClassifier(max_depth=3, max_features='log2', max_leaf_nodes=9)\n",
        "#model=KNeighborsClassifier(n_neighbors=9, weights='distance')\n",
        "\n",
        "\n",
        "#model = XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
        " #             colsample_bylevel=None, colsample_bynode=None,\n",
        "  ##           enable_categorical=False, eval_metric=None, feature_types=None,\n",
        "    #          gamma=0.5, gpu_id=None, grow_policy=None, importance_type=None,\n",
        "     #         interaction_constraints=None, learning_rate=None, max_bin=None,\n",
        "      #        max_cat_threshold=None, max_cat_to_onehot=None,\n",
        "       #       max_delta_step=None, max_depth=5, max_leaves=None,\n",
        "        #      min_child_weight=10, monotone_constraints=None,\n",
        "         #     n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
        "          #    predictor=None, random_state=None)\n",
        "model = AdaBoostClassifier(algorithm='SAMME',\n",
        "                   base_estimator=DecisionTreeClassifier(max_features='auto',\n",
        "                                                         random_state=11),\n",
        "                   learning_rate=1.04, n_estimators=4)"
      ],
      "metadata": {
        "id": "QC1QFwgZBESf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "t0=time.time()\n",
        "history = model.fit(X_train, y_train)\n",
        "print (\"training time:\", round(time.time()-t0, 3), \"s\") # the time would be round to 3 decimal in seconds\n",
        "t1=time.time()\n",
        "y_predicted = model.predict(X_test)\n",
        "print (\"predict time:\", round(time.time()-t1, 5), \"s\")"
      ],
      "metadata": {
        "id": "GIezFbiKBEQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "hr7DgVWhBENu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model score\n",
        "model.score(X_test,y_test)"
      ],
      "metadata": {
        "id": "e8rDqaIaBEI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pickle.dump(model, open(\"model_\"+model_name+str(components)+\".pkl\", \"wb\"))"
      ],
      "metadata": {
        "id": "SNOo2PRLBEEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_predicted)\n",
        "print(cm)\n",
        "%matplotlib inline\n",
        "plt.figure(figsize=(10,7))\n",
        "sns.heatmap(cm, annot=True)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Truth')\n",
        "plt.savefig(\"CM_\"+model_name+str(components)+'.png')\n"
      ],
      "metadata": {
        "id": "39IQ57qFBEBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Precision,recall,f1_score,cohen_kappa_score,auc.......\n",
        "print(\"Precision,recall,f1 score,cohen kappa score,auc.....\")\n",
        "print(\" \")\n",
        "\n",
        "# precision tp / (tp + fp)\n",
        "precision = precision_score(y_test, y_predicted)\n",
        "print('Precision: %f' % precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(y_test,y_predicted)\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = f1_score(y_test, y_predicted)\n",
        "print('F1 score: %f' % f1)\n",
        "# ROC AUC\n",
        "auc = roc_auc_score(y_test,  y_predicted)\n",
        "print('ROC AUC: %f' % auc)\n",
        "#Cohen's kappa\n",
        "kappa=cohen_kappa_score(y_test,  y_predicted)\n",
        "print('Cohen Kappa: %f' % kappa)\n",
        "print(\" \")\n",
        "\n"
      ],
      "metadata": {
        "id": "0Oj4uZzPB6UC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('MAE:', metrics.mean_absolute_error(y_test, y_predicted))\n",
        "print('MSE:', metrics.mean_squared_error(y_test, y_predicted))\n",
        "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_predicted)))\n",
        "print('VarScore:',metrics.explained_variance_score(y_test,y_predicted))"
      ],
      "metadata": {
        "id": "fxvIfSf9B_Aa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}